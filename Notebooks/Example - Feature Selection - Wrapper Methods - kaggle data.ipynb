{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feature Selection**\n",
    "Here we explore methodologies to identify which features are useful provide a higher predictive power to the model. Given a dataset, a model trained on it can depend on features directly on derived features. How do we tell wich features are the most useful? Multiple approaches exist, which are based on simple ideas of univariate analysis to complex multivariate analysis. In univariate analysis we look at how a single feature contribute to the model. Although useful, it does have pitfalls as some features are better together. In multivariate analysis we can tell which features perform well and more importantly which perform well together. Various techniques exist driven differentiated by how information is extracted. When data contains label like the case here, we use supervised techniques, nevetheless, unsupervised techniques can be used for unlabelled data.\n",
    "\n",
    "Collaborative filtering is built on the assumption that a good way to predict the\n",
    "preference of an active consumer for a target product is to find other consumers\n",
    "who have similar preferences and use their votes for that product to make a\n",
    "prediction.\n",
    "As noted in the [source page](https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/), these techniques can be classified as follows\n",
    "- **Filter methods:** based on features properties highlighted via univariate analysis\n",
    "\n",
    "- **Wrapper methods:** With a specific learning algorithm, these methose can perform a greedy search of the best feature by fitting models with possible subsets of features, assessing their quality by learning and evaluating a classifier with that feature subset. \n",
    "- **Embedded methods:** Here they aim to combine the power of both filters and wrapper while maintaining reasonable computational cost.\n",
    "- **Hybrid method:** Hybrid methods basically select features via a global transformation reduces the data to a desided number of dimensions. The new features can bear little or no resemblance to the initial features.\n",
    "\n",
    "References\n",
    "\n",
    "Libraries used in the notebook:\n",
    "* [pandas](https://pandas.pydata.org/docs/),\n",
    "* [scikit-learn](https://scikit-learn.org/stable/),\n",
    "* [optbinning](https://github.com/guillermo-navas-palencia/optbinning),\n",
    "* [sklearn.feature_selection](https://scikit-learn.org/stable/modules/feature_selection.html),\n",
    "* [category_encoders](https://contrib.scikit-learn.org/category_encoders/)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'kaggle-data.pkl', 'rb') as f:\n",
    "    df_application = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "df_application.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format columns to lower case (just for a nice look :) )\n",
    "df_application.columns = [col.lower() for col in df_application.columns]\n",
    "df_application.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working with numerical data\n",
    "X = df_application.drop('target', axis=1)\n",
    "Y = df_application.target\n",
    "numerical_columns = X.select_dtypes(include=np.number).columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifying data types for ecoding\n",
    "X = df_application.drop('target', axis=1)\n",
    "Y = df_application.target\n",
    "numerical_columns = X.select_dtypes(include=np.number).columns.values\n",
    "categorical_columns = X.select_dtypes(include=['object', 'category']).columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ce.TargetEncoder()\n",
    "X_cat = encoder.fit_transform(X[categorical_columns], Y)\n",
    "X[categorical_columns] = X_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format columns to lower case (just for a nice look :) )\n",
    "df_application.columns = [col.lower() for col in df_application.columns]\n",
    "df_application.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3 Wrapper Methods:\n",
    "Wrappers require some method to search the space of all possible subsets of features, assessing their quality by learning and evaluating a classifier with that feature subset. The feature selection process is based on a specific machine learning algorithm that we are trying to fit on a given dataset. It follows a greedy search approach by evaluating all the possible combinations of features against the evaluation criterion. The wrapper methods usually result in better predictive accuracy than filter methods.\n",
    " - Forward Feature Selection\n",
    " - Backward Feature Elimination\n",
    " - Exhaustive Feature Selection\n",
    " - Recursive Feature Elimination\n",
    " \n",
    " Forward Feature Selection: Starts with the best and gradually adds the others\n",
    " Backward Feature Elimination: Starts will all and start elimicating, with worst the first to get out\n",
    " Exhaustive Feature Selection: Brute force method that searches each of the possible combinations\n",
    " \n",
    " We are going to address each of the methods, but for now will start with Recursive Feature Elimination as the others needs mlxtend library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Recursive Feature Elimination\n",
    "\n",
    "First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute.\n",
    "\n",
    "Then, the least important features are pruned from the current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination\n",
    "rfe = feature_selection.RFE(\n",
    "    LogisticRegression(C=3, max_iter=1000, random_state=42),\n",
    "    n_features_to_select=10\n",
    ")\n",
    "rfe.fit(X.fillna(X.mean()), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.get_feature_names_out()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f55a25da613ac67f6a065c2d4cb270f12d53ae2a52e7add8c08563a4e32f1506"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('general': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
