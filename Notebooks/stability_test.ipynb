{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Similarity of Train/Val Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Test Based on Targets\n",
    "Here we want to check if they are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency, fisher_exact\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "y_train_a \n",
    "y_val_a = \n",
    "y_train_na = \n",
    "y_val_na = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contigency_non_alnc = pd.DataFrame.from_dict({\"Train\": y_train_na.groupby(\n",
    "    TARGET).size(), \"Val\": y_val_na.groupby(TARGET).size()})\n",
    "\n",
    "contigency_alnc = pd.DataFrame.from_dict({\"Train\": y_train_a.groupby(\n",
    "    TARGET).size(), \"Val\": y_val_a.groupby(TARGET).size()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    chi2_contingency(contigency_non_alnc.values), \n",
    "    \"\\n----------------------------------------\\n\",\n",
    "    chi2_contingency(contigency_alnc.values)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    fisher_exact(contigency_non_alnc.values),\n",
    "    \"\\n----------------------------------------\\n\",\n",
    "    fisher_exact(contigency_alnc.values)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test Based on Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optbinning.binning.metrics import jeffrey as psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_psi(expected, actual, buckettype='bins', buckets=10, axis=0):\n",
    "    '''Calculate the PSI (population stability index) across all variables\n",
    "\n",
    "    Args:\n",
    "       expected: numpy matrix of original values\n",
    "       actual: numpy matrix of new values, same size as expected\n",
    "       buckettype: type of strategy for creating buckets, bins splits into even splits, quantiles splits into quantile buckets\n",
    "       buckets: number of quantiles to use in bucketing variables\n",
    "       axis: axis by which variables are defined, 0 for vertical, 1 for horizontal\n",
    "\n",
    "    Returns:\n",
    "       psi_values: ndarray of psi values for each variable\n",
    "\n",
    "    Author:\n",
    "       Matthew Burke\n",
    "       github.com/mwburke\n",
    "       worksofchart.com\n",
    "    '''\n",
    "\n",
    "    def psi(expected_array, actual_array, buckets):\n",
    "        '''Calculate the PSI for a single variable\n",
    "\n",
    "        Args:\n",
    "           expected_array: numpy array of original values\n",
    "           actual_array: numpy array of new values, same size as expected\n",
    "           buckets: number of percentile ranges to bucket the values into\n",
    "\n",
    "        Returns:\n",
    "           psi_value: calculated PSI value\n",
    "        '''\n",
    "\n",
    "        def scale_range (input, min, max):\n",
    "            input += -(np.min(input))\n",
    "            input /= np.max(input) / (max - min)\n",
    "            input += min\n",
    "            return input\n",
    "\n",
    "\n",
    "        breakpoints = np.arange(0, buckets + 1) / (buckets) * 100\n",
    "\n",
    "        if buckettype == 'bins':\n",
    "            breakpoints = scale_range(breakpoints, np.min(expected_array), np.max(expected_array))\n",
    "        elif buckettype == 'quantiles':\n",
    "            breakpoints = np.stack([np.percentile(expected_array, b) for b in breakpoints])\n",
    "\n",
    "\n",
    "\n",
    "        expected_percents = np.histogram(expected_array, breakpoints)[0] / len(expected_array)\n",
    "        actual_percents = np.histogram(actual_array, breakpoints)[0] / len(actual_array)\n",
    "\n",
    "        def sub_psi(e_perc, a_perc):\n",
    "            '''Calculate the actual PSI value from comparing the values.\n",
    "               Update the actual value to a very small number if equal to zero\n",
    "            '''\n",
    "            if a_perc == 0:\n",
    "                a_perc = 0.0001\n",
    "            if e_perc == 0:\n",
    "                e_perc = 0.0001\n",
    "\n",
    "            value = (e_perc - a_perc) * np.log(e_perc / a_perc)\n",
    "            return(value)\n",
    "\n",
    "        psi_value = np.sum(sub_psi(expected_percents[i], actual_percents[i]) for i in range(0, len(expected_percents)))\n",
    "\n",
    "        return(psi_value)\n",
    "\n",
    "    if len(expected.shape) == 1:\n",
    "        psi_values = np.empty(len(expected.shape))\n",
    "    else:\n",
    "        psi_values = np.empty(expected.shape[axis])\n",
    "\n",
    "    for i in range(0, len(psi_values)):\n",
    "        if len(psi_values) == 1:\n",
    "            psi_values = psi(expected, actual, buckets)\n",
    "        elif axis == 0:\n",
    "            psi_values[i] = psi(expected[:,i], actual[:,i], buckets)\n",
    "        elif axis == 1:\n",
    "            psi_values[i] = psi(expected[i,:], actual[i,:], buckets)\n",
    "\n",
    "    return(psi_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_a = pd.read_parquet(os.path.join(path, \"m\", \"X_train.parquet\"))\n",
    "x_val_a = pd.read_parquet(os.path.join(path, \"m\", \"X_val.parquet\"))\n",
    "x_train_na = pd.read_parquet(os.path.join(path, \"f\", \"X_train.parquet\"))\n",
    "x_val_na = pd.read_parquet(os.path.join(path, \"f\", \"X_val.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_a.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_a.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_alnc_model_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_psi(x_train_a[non_alnc_model_features[0]].values, x_val_a[non_alnc_model_features[0]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSI:\n",
    "    \"\"\"Calculate the PSI (population stability index) across all variables\n",
    "\n",
    "    Args:\n",
    "       expected: numpy matrix of original values\n",
    "       actual: numpy matrix of new values, same size as expected\n",
    "       buckettype: type of strategy for creating buckets, bins splits into even splits, quantiles splits into quantile buckets\n",
    "       buckets: number of quantiles to use in bucketing variables\n",
    "       axis: axis by which variables are defined, 0 for vertical, 1 for horizontal\n",
    "\n",
    "    Returns:\n",
    "       psi_values: ndarray of psi values for each variable\n",
    "\n",
    "    Author:\n",
    "       jnsofini\n",
    "       Adapted from: https://github.com/mwburke/population-stability-index/blob/master/psi.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, expected, actual, bucket_type=\"bins\", buckets=10, axis=0):\n",
    "        self.expected = expected\n",
    "        self.actual = actual\n",
    "        self.bucket_type = bucket_type\n",
    "        self.buckets = buckets\n",
    "        self.axis = axis\n",
    "\n",
    "    def psi(self):\n",
    "        \"\"\"Calculate the PSI for a single variable\n",
    "\n",
    "        Args:\n",
    "           expected_array: numpy array of original values\n",
    "           actual_array: numpy array of new values, same size as expected\n",
    "           buckets: number of percentile ranges to bucket the values into\n",
    "\n",
    "        Returns:\n",
    "           psi_value: calculated PSI value\n",
    "        \"\"\"\n",
    "\n",
    "        breakpoints = np.arange(0, self.buckets  + 1) / (self.buckets) * 100\n",
    "\n",
    "        self.expected = self.expected\n",
    "\n",
    "        if self.bucket_type == \"bins\":\n",
    "            breakpoints = self.scale_range(\n",
    "                breakpoints, np.min(self.expected), np.max(self.expected)\n",
    "            )\n",
    "        elif self.bucket_type == \"quantiles\":\n",
    "            breakpoints = np.stack(\n",
    "                [np.percentile(self.expected, b) for b in breakpoints]\n",
    "            )\n",
    "\n",
    "        expected_percents = np.histogram(self.expected, breakpoints)[0] / len(\n",
    "            self.expected\n",
    "        )\n",
    "        actual_percents = np.histogram(self.actual, breakpoints)[0] / len(self.actual)\n",
    "\n",
    "        psi_value = sum(\n",
    "            self.sub_psi(expected_percents[i], actual_percents[i])\n",
    "            for i in range(0, len(expected_percents))\n",
    "        )\n",
    "\n",
    "        return psi_value\n",
    "\n",
    "    def get_psi(self):\n",
    "        if len(self.expected.shape) == 1:\n",
    "            psi_values = np.empty(len(self.expected.shape))\n",
    "        else:\n",
    "            psi_values = np.empty(self.expected.shape[self.axis])\n",
    "\n",
    "        for i in range(0, len(psi_values)):\n",
    "            if len(psi_values) == 1:\n",
    "                psi_values = self.psi()\n",
    "            elif self.axis == 0:\n",
    "                psi_values[i] = self.psi()\n",
    "            elif self.axis == 1:\n",
    "                psi_values[i] = self.psi()\n",
    "\n",
    "        return psi_values\n",
    "\n",
    "    @staticmethod\n",
    "    def scale_range(input, min, max):\n",
    "        input += -(np.min(input))\n",
    "        input /= np.max(input) / (max - min)\n",
    "        input += min\n",
    "        return input\n",
    "\n",
    "    @staticmethod\n",
    "    def sub_psi(e_perc, a_perc):\n",
    "        \"\"\"Calculate the actual PSI value from comparing the values.\n",
    "            Update the actual value to a very small number if equal to zero\n",
    "        \"\"\"\n",
    "        if a_perc == 0:\n",
    "            a_perc = 0.0001\n",
    "        if e_perc == 0:\n",
    "            e_perc = 0.0001\n",
    "\n",
    "        value = (e_perc - a_perc) * np.log(e_perc / a_perc)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSI(x_train_a[non_alnc_model_features[0]].values, x_val_a[non_alnc_model_features[0]].values).get_psi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the source The common interpretations of the PSI result are:\n",
    "\n",
    "- PSI < 0.1: no significant population change\n",
    "- PSI < 0.2: moderate population change\n",
    "- PSI >= 0.2: significant population change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_data = {\"feature\":[], \"psi_SB_Core\":[]}\n",
    "for col in non_alnc_model_features:\n",
    "    if x_train_na[col].dtype in [\"object\", \"category\", \"string\"]:\n",
    "        continue\n",
    "    psi_data[\"psi_SB_Core\"].append(PSI(x_train_na[col].values, x_val_na[col].values).get_psi())\n",
    "    psi_data[\"feature\"].append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(psi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = x_train_na.select_dtypes(\n",
    "            include=[\"object\", \"category\", \"string\"]\n",
    "        ).columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_psi(actual, expected):\n",
    "    index1, actual_freq = np.unique(actual, return_counts=True)\n",
    "    index2, expected_freq = np.unique(expected, return_counts=True)\n",
    "    data = pd.merge(\n",
    "        pd.DataFrame({\"index\": index1, \"actual\": actual_freq}),\n",
    "        pd.DataFrame({\"index\": index2, \"expected\": expected_freq}),\n",
    "        on=\"index\",\n",
    "        # how=\"outer\"\n",
    "        )\n",
    "\n",
    "    data[\"actual%\"] = data[\"actual\"]/data[\"actual\"].sum()\n",
    "    data[\"expected%\"] = data[\"expected\"]/data[\"expected\"].sum()\n",
    "\n",
    "    def sub_psi(e_perc, a_perc):\n",
    "        \"\"\"Calculate the actual PSI value from comparing the values.\n",
    "            Update the actual value to a very small number if equal to zero\n",
    "        \"\"\"\n",
    "        if a_perc == 0:\n",
    "            a_perc = 0.0001\n",
    "        if e_perc == 0:\n",
    "            e_perc = 0.0001\n",
    "\n",
    "        value = (e_perc - a_perc) * np.log(e_perc / a_perc)\n",
    "        return value\n",
    "\n",
    "    data[\"PSI\"] = data[[\"actual%\", \"expected%\"]].apply(lambda x: sub_psi(x[\"actual%\"], x[\"expected%\"]), axis=1)\n",
    "\n",
    "    return data[\"PSI\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_psi(x_train_na['B1_FARM_LOCN_PROV_CD'].values, x_val_na['B1_FARM_LOCN_PROV_CD'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_data = {\"feature\":non_alnc_model_features, \"psi_SB_Core\": []}\n",
    "for col in alnc_model_features:\n",
    "    if x_train_na[col].dtype in [\"object\", \"category\", \"string\"]:\n",
    "        psi_score = categorical_psi(x_train_a[col].values, x_val_a[col].values)\n",
    "    else:\n",
    "        psi_score = PSI(x_train_a[col].values, x_val_a[col].values).get_psi()\n",
    "    # psi_data[\"feature\"].append(col)\n",
    "    psi_data[\"psi_SB_Core\"].append(psi_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_table_na = pd.DataFrame.from_dict(psi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_table_a = pd.DataFrame.from_dict(psi_data).rename(columns={\"psi_SB_Core\":\"psi_alnc\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_table_all = pd.merge(psi_table_na, psi_table_a, on=\"feature\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_table_all.to_csv(\"stability-test-train-val_split.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a random forest on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_direct_lending = pd.concat([x_train_na.assign(target=0), x_val_na.assign(target=1)], axis=0)\n",
    "X_train_alnc = pd.concat([x_train_a.assign(target=0), x_val_a.assign(target=1)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train_SB_core[non_alnc_model_features]\n",
    "y = X_train_SB_core['target'].values\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "model = RandomForestClassifier(n_jobs=-1, max_depth=5, min_samples_leaf = 5)\n",
    "predictions = np.zeros(y.shape) #creating an empty prediction array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = None\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "    print(fold, t:=train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=20, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_test = X.filter(items=train_idx, axis=0), X.filter(items=test_idx, axis=0)\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    " \n",
    "    model.fit(X_train, y_train)\n",
    "    probs = model.predict_proba(X_test)[:, 1] #calculating the probability\n",
    "    predictions[test_idx] = probs"
   ]
  }
 ],
 "folders": [
  {
   "path": "../.."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('eda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949509eaf86eca253b4c01c43139ca80f6259b233ef171e78764e29ed689f836"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "settings": {}
}
